"""
ğŸ¦™ SIMPLE OLLAMA QUERY TEST
============================
Quick test of Ollama with document retrieval
"""

from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

def simple_ollama_rag():
    print("=" * 70)
    print("  ğŸ¦™ SIMPLE OLLAMA RAG TEST")
    print("=" * 70)
    print()
    print("ğŸ’° Cost: $0 | ğŸ”’ Privacy: 100% Local | âš¡ Model: Mistral")
    print()
    
    # Step 1: Initialize Ollama
    print("ğŸ“¦ Step 1: Initializing Ollama...")
    llm = OllamaLLM(model="mistral", temperature=0.1)
    embeddings = OllamaEmbeddings(model="mistral")
    print("âœ“ Ollama ready!\n")
    
    # Step 2: Create knowledge base
    print("ğŸ“š Step 2: Creating knowledge base...")
    documents = [
        "Machine learning is a subset of AI that enables systems to learn from data.",
        "Deep learning uses neural networks with multiple layers for complex pattern recognition.",
        "Natural language processing helps computers understand human language.",
        "Computer vision enables machines to interpret visual information.",
        "Reinforcement learning trains agents through rewards and penalties.",
        "Supervised learning uses labeled data to train predictive models.",
        "Unsupervised learning finds patterns in unlabeled data.",
        "Transfer learning applies knowledge from one task to another.",
        "Neural networks are inspired by biological brain structures.",
        "Gradient descent optimizes model parameters to minimize loss."
    ]
    
    # Convert to Document objects
    docs = [Document(page_content=text) for text in documents]
    print(f"âœ“ Created {len(docs)} documents\n")
    
    # Step 3: Create vector store
    print("ğŸ” Step 3: Creating vector database...")
    print("   (Generating embeddings with Ollama - may take a moment)...")
    vectorstore = FAISS.from_documents(docs, embeddings)
    print("âœ“ Vector database ready!\n")
    
    # Step 4: Test queries
    print("=" * 70)
    print("  TESTING FREE RAG QUERIES")
    print("=" * 70)
    
    queries = [
        "What is machine learning?",
        "Explain deep learning",
        "What is reinforcement learning?"
    ]
    
    for i, query in enumerate(queries, 1):
        print(f"\n[Query {i}/{len(queries)}]")
        print("=" * 70)
        print(f"â“ {query}\n")
        
        # Retrieve relevant documents
        print("ğŸ” Retrieving relevant documents...")
        relevant_docs = vectorstore.similarity_search(query, k=3)
        print(f"âœ“ Found {len(relevant_docs)} relevant documents\n")
        
        # Create context
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # Generate answer with Ollama
        print("ğŸ¤– Generating answer with Mistral (FREE!)...")
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer (be concise and factual):"""
        
        answer = llm.invoke(prompt)
        
        print(f"\nğŸ’¡ ANSWER:")
        print("-" * 70)
        import textwrap
        print(textwrap.fill(answer.strip(), width=70))
        print("-" * 70)
        
        print(f"\nğŸ“Š Documents used: {len(relevant_docs)}")
    
    # Summary
    print("\n" + "=" * 70)
    print("  ğŸ‰ TEST COMPLETE!")
    print("=" * 70)
    print(f"\nâœ… Processed {len(queries)} queries successfully")
    print(f"ğŸ’° Total cost: $0 (100% FREE!)")
    print(f"ğŸ”’ All data processed locally")
    print(f"âš¡ Real LLM responses from Mistral")
    
    print("\n" + "=" * 70)
    print("  YOU NOW HAVE FREE RAG!")
    print("=" * 70)
    print("  âœ“ No API costs")
    print("  âœ“ No rate limits")
    print("  âœ“ Complete privacy")
    print("  âœ“ Unlimited queries")
    print("  âœ“ Real LLM quality")
    print("\n  Train and experiment freely! ğŸš€")
    print("=" * 70)

if __name__ == "__main__":
    simple_ollama_rag()
